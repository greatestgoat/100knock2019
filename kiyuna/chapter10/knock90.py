'''
90. word2vecによる学習
81で作成したコーパスに対してword2vecを適用し，単語ベクトルを学習せよ．
さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．
'''
import sys
import time
import word2vec
from sklearn.metrics.pairwise import cosine_similarity


class Timer(object):
    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, *args):
        self.end = time.time()
        self.secs = self.end - self.start
        self.msecs = self.secs * 1000


def message(text="", CR=False):
    text = "\r" + text if CR else text + "\n"
    sys.stderr.write("\33[92m" + text + "\33[0m")


def train(output):
    """ 実行例
    Starting training using file ../chapter09/out81.txt
    Vocab size: 85419
    Words in train file: 11941521
    Alpha: 0.000007  Progress: 99.99%  Words/thread/sec: 153.57k
    [+] elapsed time = 2 m 7 s
    """
    with Timer() as t:
        corpus = '../chapter09/out81.txt'   # United Kingdom -> United_Kingdom
        word2vec.word2vec(corpus, output, size=300,
                          threads=32, binary=1, verbose=1)
    message(f'\n[+] elapsed time = {t.secs / 60:.0f} m {t.secs % 60:.0f} s')


if __name__ == "__main__":
    output = './out90.bin'
    try:
        message('[*] load a model')
        model = word2vec.load(output)
    except Exception as e:
        message('[-] failed to load a model')
        message('[*] training ...')
        train(output)
        message('[+] word2vec.word2vec(.) のみ実行しました')
        exit()

    print('\n--- 86 ---\n')
    message('[*] knock86')
    print(model["United_States"])

    print('\n--- 87 ---\n')
    message('[*] knock87')
    print(cosine_similarity([model["United_States"]], [model["U.S"]]))
    print(model.distance("United_States", "U.S"))

    print('\n--- 88 ---\n')
    message('[*] knock88')
    indexes, metrics = model.similar("England")
    for word, cs in model.generate_response(indexes, metrics).tolist():
        print(f'{word:<12}{cs:f}')

    print('\n--- 89 ---\n')
    message('[*] knock89')
    indexes, metrics = model.analogy(pos=["Spain", "Athens"], neg=["Madrid"])
    tgt = [model["Spain"] - model["Madrid"] + model["Athens"]]
    for idx, _ in zip(indexes, metrics):
        word = model.vocab[idx]
        cs = cosine_similarity([model[word]], tgt)[0][0]
        print(f'{word:<12}{cs:f}')


'''
* word2vec モジュール
- https://pypi.org/project/word2vec/
- https://nbviewer.jupyter.org/github/danielfrg/word2vec/tree/master/word2vec/
'''


''' RESULT

--- 86 ---

[ 0.0795533   0.07478939 -0.02472952 -0.02588838 -0.04100655 -0.01832807
 -0.0055505   0.00122942 -0.00138933  0.04862135  0.05526768 -0.05692
  0.01729056  0.03189657 -0.06756965  0.05419163 -0.0068064   0.08040772
  0.05371098 -0.10435865  0.0053054   0.01603676  0.03830428 -0.05854325
  0.06505865 -0.05742207 -0.00775725 -0.01499206  0.12213786 -0.0316522
  0.01218633  0.06277724 -0.04242324  0.04738983 -0.04495412  0.04512914
 -0.03603946  0.12611224 -0.03185159 -0.05180144 -0.05907911  0.03488704
  0.00827899 -0.01470017  0.09254903 -0.01935241 -0.04209385  0.03107227
  0.00017808  0.09950194  0.01536219 -0.05273632 -0.02519023 -0.06796911
  0.04010355 -0.00832437 -0.11391734 -0.01445965  0.02514203 -0.08239751
 -0.02488749  0.0080418   0.01185634  0.13448644 -0.0217928   0.04262855
 -0.01757471 -0.07081311 -0.0636587   0.01504597  0.04667155 -0.07598968
  0.088708   -0.08265212  0.05291926  0.06830014 -0.03384424  0.05006021
 -0.07576939 -0.05622247 -0.06930335 -0.06921008 -0.00147039 -0.03142364
 -0.01384269 -0.03518668 -0.11119648  0.05386599  0.07845494  0.0655386
 -0.13978541  0.0091287   0.04901112  0.04474865  0.05262124 -0.02301543
 -0.00426709  0.02905193  0.09279325 -0.11204987  0.00752167 -0.05832167
 -0.05146566 -0.06567742  0.07204688  0.03598443 -0.0237729   0.04949962
 -0.11341416 -0.02227571  0.02952211  0.02622335 -0.05101281  0.08806222
  0.06634378  0.0704444   0.01373472  0.00047762  0.01226066 -0.0297755
  0.01196373 -0.04962621 -0.00903176 -0.00677418  0.05103474  0.04561907
 -0.03076632  0.02175247  0.02440638 -0.03147457  0.04129768  0.05384984
 -0.02932763  0.1284394  -0.11042875 -0.01610934 -0.04182125 -0.03352317
 -0.07807122  0.05080208  0.06538954  0.03670319 -0.07410531  0.04374348
 -0.06687618  0.04198264  0.05780436  0.14088473 -0.01909846 -0.03605949
  0.10330363  0.00279111  0.07851906  0.03733965 -0.01823298 -0.07626355
 -0.06778736  0.04942872 -0.00462354  0.00459982 -0.00482776 -0.0826896
  0.00240714 -0.05214085  0.03234505 -0.06302227  0.03574935 -0.00875505
  0.01485397  0.0412794   0.01110474  0.03252691  0.17166889 -0.06292712
  0.03346315  0.01200054 -0.04122711  0.03196922  0.00719202  0.05075957
 -0.07732571  0.05372946 -0.04037543  0.13652417 -0.04380981 -0.00177047
 -0.015835    0.03978848 -0.03693919  0.07245199  0.00588088  0.05338558
  0.03904225  0.04797556 -0.06665628  0.0095148   0.04332286 -0.05062985
 -0.00947947 -0.02250944 -0.05840354 -0.00508444 -0.04875027  0.03784215
  0.11384951  0.07872445 -0.12525129  0.08812016 -0.09018085 -0.00649894
  0.08049969  0.07692971  0.03960302 -0.12853858  0.00506358 -0.1636662
 -0.0034079   0.07146661 -0.00434432 -0.01603094  0.01266183 -0.04760905
  0.02482063 -0.02731214 -0.08252002  0.01700077 -0.01016676  0.07064818
  0.01539224 -0.01541578  0.01989314  0.06645706  0.06734829  0.06667196
  0.0374704   0.04585402  0.03682908  0.08144278  0.01744531  0.02760626
  0.12580426  0.04829667  0.03767022  0.03566825 -0.04898177 -0.02976933
 -0.03498027 -0.01584807  0.00631284  0.13493678 -0.03547012  0.09502978
 -0.05594135 -0.00323669  0.10087752  0.04237348  0.00031038  0.03478921
 -0.03286608 -0.05991352 -0.08206614  0.05830375  0.1073018   0.01237612
 -0.01271257 -0.04744301 -0.00264166  0.00036262 -0.05523239 -0.02863181
 -0.05119636  0.00846622  0.05639384 -0.01729103  0.01475497 -0.03168888
 -0.08567298  0.00855719  0.01909496  0.00787321  0.09186723 -0.10502142
  0.0165404   0.14674771  0.02392704 -0.0688123   0.041199   -0.01402984
  0.08258042 -0.08418225 -0.04016605 -0.09013022 -0.03233741 -0.02022104
  0.0170594   0.02839926 -0.03800942 -0.02456201 -0.01357191 -0.07560707]

--- 87 ---

[[0.84529542]]
[('United_States', 'U.S', 0.8452953998525707)]

--- 88 ---

Wales       0.785457
Scotland    0.754680
Ireland     0.657363
Britain     0.632701
London      0.583623
Sweden      0.582929
Hampshire   0.582686
Spain       0.573824
Germany     0.564593
Liverpool   0.560405

--- 89 ---

Egypt       0.792652
Denmark     0.786592
Austria     0.769622
Greece      0.766057
Italy       0.760393
Russia      0.758277
Norway      0.747631
Turkey      0.739024
Romania     0.731276
Belgium     0.731062
'''
